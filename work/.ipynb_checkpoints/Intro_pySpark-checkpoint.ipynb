{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Créer une session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Lire de la donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Lecture brute\n",
    "\n",
    "Chargez le fichier ville_1.csv dans une variable nommée df.\n",
    "\n",
    "Vous pouvez afficher votre donnée en utilisant la méthode take() ou la methode collect() de l'objet pyspark DataFrame (attention appeler collect() sur un dataframe est déconseillé si vous avez du vrai big data).\n",
    "\n",
    "L'objet possède aussi un attribut appelé dtypes, appelez cet attribut pour obtenir la liste des colonnes et leur type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'string'),\n",
       " ('_c10', 'string'),\n",
       " ('_c11', 'string'),\n",
       " ('_c12', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/Villes/ville_1.csv\"\n",
    "df = spark.read.load(path, format=\"csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Lecture avec les entêtes\n",
    "\n",
    "Recharger le même fichier mais cette fois-ci utilisez l'option header pour rajouter les noms de colonnes à votre df.\n",
    "\n",
    "Appelez l'attribut dtypes et comparez la sortie avec celle de la lecture brute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('vitesse_a_pied', 'string'),\n",
       " ('vitesse_a_velo', 'string'),\n",
       " ('home', 'string'),\n",
       " ('travail', 'string'),\n",
       " ('sportif', 'string'),\n",
       " ('casseur', 'string'),\n",
       " ('statut', 'string'),\n",
       " ('salaire', 'string'),\n",
       " ('sexe', 'string'),\n",
       " ('age', 'string'),\n",
       " ('sportivite', 'string'),\n",
       " ('velo_perf_minimale', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df = spark.read.format('csv').options(header=True).load(path)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Lecture avec les types détectés automatiquement\n",
    "\n",
    "Recharger le fichier avec  l'option inferShema.\n",
    "\n",
    "L'option 'inferSchema' permet de transformer les colonnes en types plus précis : entier  / booléens / chaines de caractères... bien sûr spark trouve les types uniquement si le fichier d'origine permet de les trouver de manière simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('vitesse_a_pied', 'double'),\n",
       " ('vitesse_a_velo', 'double'),\n",
       " ('home', 'string'),\n",
       " ('travail', 'string'),\n",
       " ('sportif', 'boolean'),\n",
       " ('casseur', 'boolean'),\n",
       " ('statut', 'string'),\n",
       " ('salaire', 'double'),\n",
       " ('sexe', 'string'),\n",
       " ('age', 'int'),\n",
       " ('sportivite', 'double'),\n",
       " ('velo_perf_minimale', 'double')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv').options(header=True, inferSchema=True).load(path)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, vitesse_a_pied=2.344207322136085, vitesse_a_velo=6.251219525696226, home='(lon:2.62 lat:2.59)', travail='(lon:2.34 lat:0.97)', sportif=False, casseur=False, statut='cadre', salaire=135167.36710061165, sexe='H', age=44, sportivite=7.814024407120282, velo_perf_minimale=0.4),\n",
       " Row(id=3, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:3.79 lat:3.81)', travail='(lon:0.20 lat:0.21)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=20026.72646423192, sexe='F', age=20, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=4, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:3.39 lat:0.93)', travail='(lon:0.58 lat:0.20)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=15214.584161640825, sexe='F', age=35, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=5, vitesse_a_pied=0.030000000000000006, vitesse_a_velo=0.08, home='(lon:2.88 lat:3.78)', travail='(lon:3.38 lat:2.93)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18235.92844960795, sexe='H', age=71, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=6, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:0.44 lat:1.45)', travail='(lon:1.85 lat:0.04)', sportif=False, casseur=False, statut='professeur', salaire=30852.120709809133, sexe='F', age=79, sportivite=0.1, velo_perf_minimale=0.4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) L'attribut schema\n",
    "\n",
    "Il vous permet d'afficher le schéma de votre df, avec pour chaque colonne son nom, son type, et si elle accepte les valeurs nulles ou non. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('vitesse_a_pied', DoubleType(), True), StructField('vitesse_a_velo', DoubleType(), True), StructField('home', StringType(), True), StructField('travail', StringType(), True), StructField('sportif', BooleanType(), True), StructField('casseur', BooleanType(), True), StructField('statut', StringType(), True), StructField('salaire', DoubleType(), True), StructField('sexe', StringType(), True), StructField('age', IntegerType(), True), StructField('sportivite', DoubleType(), True), StructField('velo_perf_minimale', DoubleType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', id='50', vitesse_a_pied='50', vitesse_a_velo='50', home='50', travail='50', statut='50', salaire='50', sexe='50', age='50', sportivite='50', velo_perf_minimale='50'),\n",
       " Row(summary='mean', id='26.5', vitesse_a_pied='0.4392468502791733', vitesse_a_velo='1.145550067289242', home=None, travail=None, statut=None, salaire='40143.21200936129', sexe=None, age='48.4', sportivite='1.721904835482771', velo_perf_minimale='0.39999999999999986'),\n",
       " Row(summary='stddev', id='14.577379737113251', vitesse_a_pied='0.46957482160828645', vitesse_a_velo='1.2384549280855548', home=None, travail=None, statut=None, salaire='32900.934492297645', sexe=None, age='19.79898987322333', sportivite='1.7739858396475683', velo_perf_minimale='0.0'),\n",
       " Row(summary='min', id='2', vitesse_a_pied='0.02', vitesse_a_velo='0.05', home='(lon:0.01 lat:2.24)', travail='(lon:0.20 lat:0.21)', statut='cadre', salaire='9511.945356442959', sexe='F', age='16', sportivite='0.1', velo_perf_minimale='0.4'),\n",
       " Row(summary='25%', id='14', vitesse_a_pied='0.030000000000000006', vitesse_a_velo='0.08', home=None, travail=None, statut=None, salaire='19383.17526928032', sexe=None, age='34', sportivite='0.1', velo_perf_minimale='0.4'),\n",
       " Row(summary='50%', id='26', vitesse_a_pied='0.2944550644296354', vitesse_a_velo='0.7852135051456944', home=None, travail=None, statut=None, salaire='29085.686473822872', sexe=None, age='49', sportivite='1.0946509589072042', velo_perf_minimale='0.4'),\n",
       " Row(summary='75%', id='39', vitesse_a_pied='0.6295531740219638', vitesse_a_velo='1.579551277776979', home=None, travail=None, statut=None, salaire='50740.88071802778', sexe=None, age='63', sportivite='2.965581059005121', velo_perf_minimale='0.4'),\n",
       " Row(summary='max', id='51', vitesse_a_pied='2.344207322136085', vitesse_a_velo='6.251219525696226', home='(lon:3.90 lat:0.62)', travail='(lon:3.91 lat:3.22)', statut='éboueur', salaire='148702.7189509448', sexe='H', age='83', sportivite='7.814024407120282', velo_perf_minimale='0.4')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez aussi la méthode printSchema() qui permet d'afficher le shéma du df de manière plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ecriture de la dataframe sur le disque\n",
    "\n",
    "Sauvegardez le df sous différents formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) choix du format : csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").save(\"./data/Villes/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) choix du format : parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"./data/Villes/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) choix du format : json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save(\"./data/Villes/ville\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Lecture de différents formats\n",
    "\n",
    "Vous pouvez choisir de lire le df sous un format ou un autre en utilisant l'argument format dans la fonction spark.read.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-e7a38c5b-7887-44c6-93d0-c7137159e314-c000.json  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# le ! vous permet d'executer des commandes dans votre terminal depuis le notebook\n",
    "!ls ./data/Villes/ville/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.load(\"./data/Villes/ville/\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.load(\"./data/Villes/parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Calculer des résultats : les actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Nombre de lignes : count\n",
    "\n",
    "Chargez les fichiers csv contenus dans le dossiers ./data/Cyclistes/ dans un df nommé cyclistes, puis comptez les lignes du dataframe obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes = spark.read.load(\"./data/Cyclistes/\", format=\"csv\", header=True, inferSchema=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 1), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 2), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 3), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 4), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 5), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher le schéma de ce nouveau df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sur_velo: boolean (nullable = true)\n",
      " |-- velo: string (nullable = true)\n",
      " |-- vitesse: double (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- destination_finale: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez 10 lignes du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 1), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 2), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 3), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 4), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 5), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 6), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 7), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 8), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 9), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id=12, timestamp=datetime.datetime(2018, 1, 1, 0, 10), sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2) Moyenne : agg + colonne + mean\n",
    "\n",
    "A l'aide de la méthode agg(), calculez la moyenne sur la colonne vitesse.\n",
    "\n",
    "Vous pouvez récuperer le résultat avec la méthode collect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = cyclistes.groupby([\"id\"]).agg({\"vitesse\":\"mean\"}).collect()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Quantile approximatifs pour gagner du temps de calcul\n",
    "\n",
    "En statistiques et en théorie des probabilités, les quantiles sont les valeurs qui divisent un jeu de données en intervalles contenant le même nombre de données. Il y a donc un quantile de moins que le nombre de groupes créés. Ainsi les quartiles sont les trois quantiles qui divisent un ensemble de données en quatre groupes de taille égale.\n",
    "\n",
    "La méthode approxQuantile permet de laisser une tolérance a l'erreur ce qui réduit le temps de calul sur d'énormes jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_quantile(df, erreur_acceptee):\n",
    "    debut            = time.time()\n",
    "    colonne          = \"vitesse\"\n",
    "    quantiles_voulus = [0.25, 0.50, 0.75]\n",
    "    resultat         =  df.approxQuantile(colonne, quantiles_voulus , erreur_acceptee )\n",
    "    fin              = time.time()\n",
    "    delais           = fin -debut\n",
    "    print (\"delais =%.2f sec, quantiles = %s\"%(delais, resultat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =17.92 sec, quantiles = [0.030000000000000006, 0.2973786407359121, 0.6467173919199576]\n"
     ]
    }
   ],
   "source": [
    "calcul_quantile(cyclistes, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =11.23 sec, quantiles = [0.030000000000000006, 0.2973786407359121, 0.6295531740219638]\n"
     ]
    }
   ],
   "source": [
    "calcul_quantile(cyclistes, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =153.09 sec, quantiles = [0.030000000000000006, 0.3283952876721612, 0.6295531740219638]\n"
     ]
    }
   ],
   "source": [
    "calcul_quantile(cyclistes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload de la dataframe villes\n",
    "\n",
    "Chargez le fichier villes dans un df nommé villes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes =spark.read.load(\"./data/Villes/\", format=\"csv\", header=True, inferSchema=\"True\")\n",
    "villes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, vitesse_a_pied=2.344207322136085, vitesse_a_velo=6.251219525696226, home='(lon:2.62 lat:2.59)', travail='(lon:2.34 lat:0.97)', sportif=False, casseur=False, statut='cadre', salaire=135167.36710061165, sexe='H', age=44, sportivite=7.814024407120282, velo_perf_minimale=0.4),\n",
       " Row(id=3, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:3.79 lat:3.81)', travail='(lon:0.20 lat:0.21)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=20026.72646423192, sexe='F', age=20, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=4, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:3.39 lat:0.93)', travail='(lon:0.58 lat:0.20)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=15214.584161640825, sexe='F', age=35, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=5, vitesse_a_pied=0.030000000000000006, vitesse_a_velo=0.08, home='(lon:2.88 lat:3.78)', travail='(lon:3.38 lat:2.93)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18235.92844960795, sexe='H', age=71, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=6, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:0.44 lat:1.45)', travail='(lon:1.85 lat:0.04)', sportif=False, casseur=False, statut='professeur', salaire=30852.120709809133, sexe='F', age=79, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=7, vitesse_a_pied=0.9726853575358816, vitesse_a_velo=2.431713393839704, home='(lon:0.18 lat:2.60)', travail='(lon:2.16 lat:2.28)', sportif=False, casseur=False, statut='employe', salaire=54494.87538632937, sexe='F', age=46, sportivite=4.863426787679408, velo_perf_minimale=0.4),\n",
       " Row(id=8, vitesse_a_pied=0.24473592541818706, vitesse_a_velo=0.6526291344484989, home='(lon:1.67 lat:0.17)', travail='(lon:2.19 lat:1.90)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=33597.687351868175, sexe='H', age=63, sportivite=0.8157864180606236, velo_perf_minimale=0.4),\n",
       " Row(id=9, vitesse_a_pied=0.6295531740219638, vitesse_a_velo=1.5738829350549093, home='(lon:2.16 lat:2.32)', travail='(lon:3.44 lat:3.54)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=11799.927798039249, sexe='F', age=65, sportivite=3.1477658701098186, velo_perf_minimale=0.4),\n",
       " Row(id=10, vitesse_a_pied=0.3385617496874321, vitesse_a_velo=0.9028313324998192, home='(lon:2.16 lat:2.98)', travail='(lon:2.34 lat:2.56)', sportif=False, casseur=False, statut='professeur', salaire=33131.88170893892, sexe='H', age=32, sportivite=1.128539165624774, velo_perf_minimale=0.4),\n",
       " Row(id=11, vitesse_a_pied=0.25214291732243804, vitesse_a_velo=0.630357293306095, home='(lon:0.28 lat:2.44)', travail='(lon:1.28 lat:1.40)', sportif=False, casseur=False, statut='employe', salaire=53716.3729157698, sexe='F', age=21, sportivite=1.26071458661219, velo_perf_minimale=0.4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) corrélation\n",
    "\n",
    "En probabilités et en statistique, la corrélation entre plusieurs variables aléatoires ou statistiques est une notion de liaison qui contredit leur indépendance.\n",
    "\n",
    "Calculez la corrélation entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) covariance\n",
    "\n",
    "La covariance entre deux variables aléatoires est un nombre permettant de quantifier leurs écarts conjoints par rapport à leurs espérances respectives. Elle s’utilise également pour deux séries de données numériques (écarts par rapport aux moyennes).\n",
    "La covariance est une extension de la notion de variance. La corrélation est une forme normalisée de la covariance.\n",
    "\n",
    "Calculez la covariance entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6) sample\n",
    "\n",
    "La méthode sample() permet de tirer aléatoirement une fraction du dataframe.\n",
    "Stockez dans un nouveau dataframe nommée villes_1pct, une fraction egale à 1% du df. Comptez le nombre de lignes obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la méthode exceptAll(), compter le nombre de ligne dans ville en omettant la fraction contenu dans ville_1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7) filter \n",
    "\n",
    "La méthode filter() permet le df selon certaines valeurs dans les colonnes.\n",
    "\n",
    "Utilisez cette méthode pour récuperer seulement les lignes avec le sexe féminin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peux aussi filtrer le df avec la méthode where(). Filtrez le df de la même façon que precedemment en utilisant cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Transformer la données : les transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : demandent à être suivi par un collect ou une action (count par exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Obtenir des statistiques sur les colonnes numériques\n",
    "\n",
    "La méthode describe() permet de calculer les statistiques récapitulatives d'une ou plusieurs colonnes numériques dans un df. Si le nom des colonnes n'est pas spécifié, la méthode calculera des statistiques récapitulatives pour toutes les colonnes numériques présentes dans le df.\n",
    "\n",
    "Afficher les statistiques de la colonne age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) groupby\n",
    "\n",
    "La méthode groupBy() suivie de la methode agg() permet de grouper le df selon les catgories d'une ou plusieurs colonnes pour faire des calculs sur ces catégories.\n",
    "\n",
    "Calculez la moyenne de la colonnes sportivité selon le sexe des personnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne de la colonne age et la valeur max de la colonne sportivité par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne des colonnes vitesse_a_pied et vitesse_a_velo par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) summary\n",
    "\n",
    "La méthode summary() permet des faire des calculs statistiques de base sur toutes les colonnes du df.\n",
    "\n",
    "Appliquez un count et un max sur toutes les colonnes du df et afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4) Union de dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajouter les colonnes les unes à côté des autres : join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes.join(villes, on=\"id\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajouter les lignes les unes sous les autres : union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.unionByName(villes).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6) Concaténation de colonne : F.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici reprendre le df cyclistes.\n",
    "\n",
    "Utiliser les méthodes withColumn() et F.concat() pour ajouter une colonne au df qui contiendra la concatenation des valeurs des colonnes id et sur_velo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/Cyclistes/*.csv\" \n",
    "cyclistes = spark.read.format(\"csv\").option(\"header\", \"true\").load(path, inferSchema=True)\n",
    "cyclistes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Fonctions udf \n",
    "Il est possible d'enregistrer des fonctions python que l'on écrit nous même pour les appliquer sur une colonne d'une dataframe, c'est ce qu'on appelle les udf, pour User Defined Functions.\n",
    "\n",
    "Voici une fonction qui prend en argument une colonne et calcule le carré des valeurs de cette colonne.\n",
    "Appliquez cette fonction sur la colonne salaire de votre df. Affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType = FloatType())\n",
    "def cube(colonne):\n",
    "    return colonne*colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
